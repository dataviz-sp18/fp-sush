---
title: "Concepts in Machine Learning: Class Imbalance"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    social: menu
    source: embed

runtime: shiny
---

```{r}
library(ggplot2)
library(dplyr)
library(flexdashboard)
library(gridExtra)
library(plotly)
library(shiny)
library(rsconnect)
library(ggpubr)


# define function to load data in an appropriate format
load_data <- function(xfile, yfile){
  X <- read.csv(xfile)
  y <- read.csv(yfile)
  
  df <- as.data.frame(X$X0)
  colnames(df) <- c('x1')
  df$x2 <- X$X1
  df$y <- y$X0  
  
  return(df)
}


# define function to plot data 

plot_basic <- function(df,title){
  g <- ggplot(df, aes(x = x1, y = x2, color = y)) +
    ggtitle(title) +
    geom_point(show.legend = FALSE) +
    xlim(-5,5) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
         axis.ticks.y=element_blank())
  return(g)
}

plot_jitter <- function(df,title){
  g <- ggplot(df, aes(x = x1, y = x2, color = y)) +
    ggtitle(title) +
    xlim(-5,5) +
    geom_point(show.legend = FALSE) +
    geom_jitter(width = 0.1, height = 0.1, show.legend = FALSE) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) 
  return(g)
}

plot_polygon <- function(df, title){
  ggplot(df, aes(x = x1, y = x2, color = y)) +
    geom_point(show.legend = FALSE)+
    stat_chull( data = negatives, mapping = aes(x = x1, y = x2), alpha = 0.2, geom = "polygon", linetype = 0, show.legend = FALSE)+
    ggtitle(title) +
    geom_point(show.legend = FALSE) +
    xlim(-5,5) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())
  }

plot_tomek <- function(df, highlight,title){
g <- ggplot(df, aes(x = x1, y = x2, color = y)) +
    geom_point(show.legend = FALSE)+
    ggtitle(title) +
    geom_point(show.legend = FALSE) +
    xlim(-5,5) +
    geom_text(data=highlight, label="Tomek Link", vjust=-1, hjust = 0.2, show.legend = FALSE)+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())
return(g)
  }

# load datasets
df <- load_data('Imbalanced Data/X.csv','Imbalanced Data/y.csv')
smote <- load_data('Imbalanced Data/smote.csv', 'Imbalanced Data/smote_y.csv')
tomek <- load_data('Imbalanced Data/tomek.csv','Imbalanced Data/tomek_y.csv')

# separate positives and negatives
positives <- df %>% filter(y == 1)
negatives <- df %>% filter(y == 0)

# create random undersampled and oversampled datasets
random_oversampled <- rbind(positives, sample_n(negatives, 180, replace = TRUE))


# identify tomek links by hand
link1 <- negatives %>% filter(x1 > -3.028886 & x2 > -0.38917162)
link2 <- df %>% filter(x1 > -2.089825) %>% filter(y == 0)
link3 <- negatives %>% filter(x2 < -1.9)

highlight <- rbind(link2, link1[c(2), ],link3[c(2), ])

```

<!-- this changes the size of the frames in the storyboard and shifts the arrows to the right --> 

<style>

.storyboard-nav .sbframelist {
        margin: 0 auto;
        width: 94%;
        height: 50px;
        overflow: hidden;
        text-shadow: none;
        margin-bottom: 8px;
}

.storyboard-nav .sbprev, .storyboard-nav .sbnext  {
        float: right;
        width: 2%;
        height: 50px;
        font-size: 25px;
    }

</style>

### Introduction {data-commentary-width=350}

```{r}
renderPlotly({
  plot_basic(df, 'Original Imbalanced Data')
})

```

***
<p align=justify>What is class imbalance?<br><br>
A common problem encountered in classification problems is that of class imbalance. In a binary classification context, this means that one class occurs far more than the other. Majority class examples are depicted in blue while minority class examples are depicted in black. In this sample dataset, 10% of observations belong to the minority class and the remaining 90% belongs to the majority class.<br><br>
The sparseness of observations from the minority class makes it difficult for the classifier to learn to identify it accurately. Further, the metrics of model evaluation we use in balanced datasets do not work quite as well in these contexts. Consider a classifier that naively predicts 'Blue' for all examples in the dataset - since 90% of the data is indeed blue, it would achieve an accuracy of 90%. However, if we're interested in identifying the black observations, this would be useless as it would fail to identify 100% of them. <br><br>
Go on to the next slide to look at some resampling methods commonly used to address the class imbalance problem.
</p>

### Random Undersampling {data-commentary-width=350}

```{r}

resampled_positives <- sample_n(positives, 20, replace = TRUE)
random_undersampling <- rbind(resampled_positives, negatives)

renderPlot({
  input$ref
  grid.arrange(plot_basic(df, 
                        'Original Imbalanced Data'), 
             plot_basic(rbind(sample_n(positives, 20, replace = TRUE), negatives),
                        'Random Undersampling'), ncol=2)})
```

***
<p align=justify>Random Undersampling<br><br>
Random undersampling a non-heuristic resampling method where we train our classifier with a random subset of examples from the majority class and all of the data from the minority class, such that the two are equal in number. <br><br>
This method is also not robust to cross-validation because of the fact that not all small subsets of a large dataset follow the same probability distribution. Results vary significantly depending on which subset of the majority class examples are considered for training. Click 'Refresh' a few times to see how different each subset of the data can look.</p>



```{r}
actionButton("ref", label = "Refresh", style='padding:4px; font-size:80%', class = "btn btn-lg btn-primary", 
             onclick ="location.href='#section-frame3';")
```

### Random Oversampling {data-commentary-width=350}

```{r}

renderPlot({
  if(input$jitter  == TRUE){
  grid.arrange(plot_basic(df, 'Original Imbalanced Data'), plot_jitter(random_oversampled, 'Random Oversampling'), ncol=2)
  }
  else{
  grid.arrange(plot_basic(df, 'Original Imbalanced Data'), plot_basic(random_oversampled, 'Random Oversampling'), ncol=2)
  }
  
})


```

***
<p align=justify>Random Oversampling<br><br>
This is a non-heuristic resampling method, where we randomly sample from the minority class with replacement, until we have as many examples as there are in the majority class. An obvious drawback of this approach is that by simply replicating observations we are not actually training the classifier with more information. <br><br>
It is equivalent to up-weighting each observation from the minority class and down-weighting each observation from the majority class. This also places undue emphasis on outliers because the classifier repeatedly encounters examples that are outliers and learns that they are typical instances of the minority class, leading to poor prediction accuracy on held-out data. This approach makes the classifier overfit to a great degree, it learns to recognize the examples it has encountered very well, but fails to generalize to out-of-sample examples.<br><br>
Since the new examples are identical to the existing ones, the graph with oversampled data looks identical to one on the left with the original data. Click 'Jitter' to add a little random noise to the observations so you can see the different observations more clearly. </p>

```{r}
actionButton("jitter", label = "Jitter", style='padding:4px; font-size:80%', class = "btn btn-lg btn-primary", 
             onclick ="location.href='#section-frame3';")
```

### Synthetic Minority Oversampling {data-commentary-width=350}

```{r}
renderPlot({
  
  grid.arrange(plot_polygon(df, 'Original Imbalanced Data'), plot_polygon(smote, 'SMOTE: Synthetic Minority Oversampling'), ncol=2)

})

```


***
<p align=justify>Synthetic Minority Oversampling<br><br>
<a href="https://www.jair.org/index.php/jair/article/view/10302/">Chawla et al. (2002)</a>  came up with the Synthetic Minority Oversampling Technique (SMOTE) where we donâ€™t merely replicate examples from the minority class, but generate new, synthetic examples. This is done by interpolating between several minority class examples that lie together. Intuitively, this allows the classifier to build larger decision regions that contain nearby instances from the minority class and helps avoid overfitting to the very few examples available from the minority class.<br><br>
Notice how all the new synthetic samples lie within the same convex feature space as the original minority class samples.</p>


### Removing Tomek Links {data-commentary-width=350}

```{r}

renderPlot({
  p1<- plot_tomek(df, highlight, 'Original Imbalanced Data')
  p2 <-  plot_tomek(tomek, highlight,'Removing Tomek Links')
  grid.arrange(p1,p2, ncol=2)
  })

```


***
<p align=justify>Removing Tomek Links<br><br>
In order to help the classifier better distinguish between the two classes, <a href="https://ci.nii.ac.jp/naid/80013575533/">Tomek</a> suggests that borderline examples and those suffering from class-label noise be removed. Consider two observations $x$ and $y$, each having a different class label. Let $\delta(x, y)$ be the Euclidian distance between $x$ and $y$. Then, a pair $(x,y)$ is considered a Tomek link if and only if no other example $z$ exists such that $\delta(x,z) < \delta(x,y)$ or $\delta(y,z) < \delta(y,x)$. In other words, two observations form a Tomek link if they have different class labels and are each other's nearest neighbour. Observations that belong to Tomek links are thus considered either borderline or noisy. The idea here is that the presence of borderline or noisy examples occludes the classifier's ability to learn what an example from the minority class looks like. By removing the majority class example from each Tomek link, we create clearer decision boundaries.</p>

### Explanation 

<p align=justify>Explanation<br><br>
The purpose of my Shiny app is to examine some ways to resample data to address class imbalance problems in machine learning. The Storyboard format allows the reader to progress in order at their own pace. Each panel in the storyboard offers one method of resampling, graphs that compare original and resampled data and a written explanation. <br><br>
In the frame for Random Undersampling, I added a little button that allows users to understand the way in which this method is very dependent on the seed set for randomized undersampling. In the frame for Random Oversampling, I added a button to let users \textit{jitter} the data to see the replicated samples. In the frame for the SMOTE method, I wanted to illustrate that all newly generated synthetic samples still lie within the same convex feature space as the original minority class examples. To this end, I used the `ggpubr` library to draw a convex hull around the borders of the minority class examples. <br><br>
I think the interactivity in my app helps users systematically follow through the different resampling methods and look at subtleties for themselves.</p>


